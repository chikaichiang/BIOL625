{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def standardize_train_and_test(train_csv, test_csv, train_out_csv=None, test_out_csv=None):\n",
        "    \"\"\"\n",
        "    Standardize features in training and test datasets.\n",
        "    Fit scaler on training set features, then apply to both training and test features.\n",
        "\n",
        "    Parameters:\n",
        "        train_csv (str): Path to training CSV file.\n",
        "        test_csv (str): Path to test CSV file.\n",
        "        train_out_csv (str, optional): Path to save standardized training CSV.\n",
        "        test_out_csv (str, optional): Path to save standardized test CSV.\n",
        "\n",
        "    Returns:\n",
        "        df_train_std (pd.DataFrame): Standardized training DataFrame.\n",
        "        df_test_std (pd.DataFrame): Standardized test DataFrame.\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df_train = pd.read_csv(train_csv)\n",
        "    df_test = pd.read_csv(test_csv)\n",
        "\n",
        "    # Columns\n",
        "    id_col = df_train.columns[0]\n",
        "    taxa_col = df_train.columns[-2]\n",
        "    label_col = df_train.columns[-1]\n",
        "\n",
        "    # Extract features and meta columns from train\n",
        "    X_train = df_train.iloc[:, 1:-2]\n",
        "    train_meta = df_train[[id_col, taxa_col, label_col]]\n",
        "\n",
        "    # Extract features and meta columns from test\n",
        "    X_test = df_test.iloc[:, 1:-2]\n",
        "    test_meta = df_test[[id_col, taxa_col, label_col]]\n",
        "\n",
        "    # Fit scaler on training features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "\n",
        "    # Transform test features using the same scaler\n",
        "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "    # Recombine\n",
        "    df_train_std = pd.concat([train_meta[id_col].reset_index(drop=True),\n",
        "                              X_train_scaled.reset_index(drop=True),\n",
        "                              train_meta[[taxa_col, label_col]].reset_index(drop=True)], axis=1)\n",
        "\n",
        "    df_test_std = pd.concat([test_meta[id_col].reset_index(drop=True),\n",
        "                             X_test_scaled.reset_index(drop=True),\n",
        "                             test_meta[[taxa_col, label_col]].reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Save if paths provided\n",
        "    if train_out_csv:\n",
        "        df_train_std.to_csv(train_out_csv, index=False)\n",
        "        print(f\"✅ Saved standardized training data to {train_out_csv}\")\n",
        "    if test_out_csv:\n",
        "        df_test_std.to_csv(test_out_csv, index=False)\n",
        "        print(f\"✅ Saved standardized test data to {test_out_csv}\")\n",
        "\n",
        "    return df_train_std, df_test_std"
      ],
      "metadata": {
        "id": "pKl8feUFlcog"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def standardize_protist_with_training_scaler(train_csv, protist_csv, protist_out_csv=None):\n",
        "    \"\"\"\n",
        "    Standardize protist dataset using scaler fitted on training data.\n",
        "\n",
        "    Parameters:\n",
        "        train_csv (str): Path to standardized training CSV (with taxa + label).\n",
        "        protist_csv (str): Path to protist dataset CSV (same feature format).\n",
        "        protist_out_csv (str): Path to save standardized protist CSV.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Standardized protist DataFrame (with ID + scaled features).\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df_train = pd.read_csv(train_csv)\n",
        "    df_protist = pd.read_csv(protist_csv)\n",
        "\n",
        "    # Identify columns\n",
        "    id_col = df_train.columns[0]\n",
        "    feature_cols = df_train.columns[1:-2]  # Exclude ID, taxa, label\n",
        "\n",
        "    # Sanity check\n",
        "    missing = [col for col in feature_cols if col not in df_protist.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"❌ Protist data is missing features: {missing}\")\n",
        "\n",
        "    # Fit scaler on training features\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(df_train[feature_cols])\n",
        "\n",
        "    # Standardize protist features\n",
        "    protist_ids = df_protist[id_col]\n",
        "    X_protist = df_protist[feature_cols]\n",
        "    X_protist_scaled = pd.DataFrame(scaler.transform(X_protist), columns=feature_cols)\n",
        "\n",
        "    # Combine ID + scaled features\n",
        "    df_protist_std = pd.concat([protist_ids.reset_index(drop=True),\n",
        "                                 X_protist_scaled.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Save output\n",
        "    if protist_out_csv:\n",
        "        df_protist_std.to_csv(protist_out_csv, index=False)\n",
        "        print(f\"✅ Saved standardized protist data to {protist_out_csv}\")\n",
        "\n",
        "    return df_protist_std\n"
      ],
      "metadata": {
        "id": "EDtU2sVd2EN9"
      },
      "execution_count": 108,
      "outputs": []
    }
  ]
}